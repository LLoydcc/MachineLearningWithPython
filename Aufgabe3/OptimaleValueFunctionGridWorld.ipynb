{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7f588b-d251-4401-b383-d7311d1c61bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Erkennung einer optimalen Value-Funktion für GridWorld**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f31bf-b79d-4aaa-950b-4162a8c8247c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **GridWorld**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b40f881-ba6a-426f-80b1-0aacb2ed73e2",
   "metadata": {},
   "source": [
    "Die GridWorld beschreibt ein 2D-Array. Innerhalb der GridWorld werden ein Start- und ein Zielpunkt definiert. Neben dem  Start- und Zielpunkt kann (muss aber nicht) die GridWorld Hindernisse in Form von Blockformationen beinhalten. Das Ziel soll es nun sein mit Hilfe von Reinforcement Learning einen möglichst schnellen Weg von Start nach Ziel, durch die GridWorld zu finden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fe7ec7-1342-4dd2-ba58-08857fae611d",
   "metadata": {},
   "source": [
    "### **Value-Funktionen**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9b5d28-56f5-493f-aa2a-75b3f5ebd1e8",
   "metadata": {},
   "source": [
    "Eine Value-Funktion bestimmt bzw. beschreibt *wie gut* der aktuell verwendete Agent auf Basis seines aktuellen Standpunktes innerhalb der Umgebung mit Inbetrachtnahme seiner Entscheidungen performt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc65d6-9258-4134-aaf7-e82b70d0aeda",
   "metadata": {},
   "source": [
    "Innerhalb der GridWorld muss der Agent immer eine bestimmte Anzahl an Schritten durchführen, um zu seinem Ziel zu gelangen. Dies kann entweder sehr gut funktionieren (der Agent wählt immer einen passenden Schritt und kommt seinem Ziel Schritt für Schritt näher) oder auch sehr schlecht funktionieren, indem der Agent falsche Wege nimmt oder sich z.B. im Kreis dreht (Deadlock)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ba987-f32c-4f67-a571-ba108e522be8",
   "metadata": {},
   "source": [
    "Es ist also sinnvoll zu bestimmen ob durchgeführte Aktionen des Agenten auf Basis der Value-Funktion gut oder eher schlecht waren. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb11371-0931-4a12-bcd6-ce128e4cf6f6",
   "metadata": {},
   "source": [
    "Um prüfen zu können, ob die Value-Funktion optimal ist, wird ein Array angelegt mit dem kürzesten Weg (wenigste Schritte) vom aktuellen Index hin zum Ziel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6024616-e710-4333-a387-584b30755db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepcounts = ([\n",
    "[ 5, 4, 3, 2, 1,-2, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11],\n",
    "[ 6, 5, 4, 3, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12],\n",
    "[ 7, 6,-1, 4, 3, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13],\n",
    "[ 8, 7,-1, 5,-1,-1,-1,-1,-1, 7,-1,-1,-1,11,12,13,14],\n",
    "[ 9, 8,-1, 6, 7, 8, 9,10, 9, 8, 9,10,11,12,13,14,15],\n",
    "[10, 9,-1, 7, 8, 9,10,11,10, 9,10,11,12,13,14,15,16],\n",
    "[11,10,-1, 8, 9,10,11,12,11,10,11,12,13,14,15,16,17],\n",
    "[12,11,-1, 9,10,11,12,13,12,11,12,13,14,15,16,17,18],\n",
    "[13,12,-1,10,11,12,13,14,13,12,13,14,15,16,17,18,19],\n",
    "[14,13,12,11,12,13,14,15,14,13,14,15,16,17,18,19,20],\n",
    "[15,14,13,12,13,14,15,16,15,14,15,16,17,18,19,20,21]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a733ec6-0cb4-42ba-81cf-55ae2d7828ff",
   "metadata": {},
   "source": [
    "**-2** definiert das Ziel, **-1** Hindernise und die **sonstigen Zahlen** die Anzahl der Schritte, die benötigt werden um auf schnellsten Wege zum Ziel zu kommen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ac8e55-8e7c-44e5-be0c-b1cea9923ae9",
   "metadata": {},
   "source": [
    "Nachfolgende Funktion berechnet nun die Anzahl der Schritte der aktuell verwendeten Value-Funktion des Agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158067ce-856c-450d-8342-ae9e2c4c4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def check_optimal(self, sv):\n",
    "    result = np.zeros((self.environent.gw_y_count, self.environent.gw_x_count))  # Array mit berechneten Steps zum Ziel\n",
    "    for row in range(len(sv)):  # Schleife durch alle bereits berechneten Statevalues\n",
    "        for col in range(len(sv[row])):\n",
    "            crow = row      # Zwischenvariable der aktuellen Row\n",
    "            ccol = col      # Zwischenvariable der aktuellen Col\n",
    "            goal = False    # Zum pruefen ob das Zeil erreicht wurde\n",
    "            dead = False    # Zum pruefen ob es sich aktuell um ein Hindernis oder ein Deadlock handelt\n",
    "            steps = 0       # Zaehlvariable der angewendeten Schritte vom aktuellen Index aus zum Ziel\n",
    "            while not goal and not dead:\n",
    "                # Pruefen ob der aktuelle Index ein Hindernis ist, wenn ja, dann dead\n",
    "                if bool(self.agent.stateinfos.get(self.environent.coord2state(row, col))):\n",
    "                    # Aktueller State des Index innerhalb der Statevalues des Agents\n",
    "                    state = self.agent.stateinfos.get(self.environent.coord2state(crow, ccol))\n",
    "                    # Die vom Agent gewaehlte Aktion auf Basis der 'Greedy-Q-Policy'\n",
    "                    action = state.policy_greedy_q_based()\n",
    "                    # Aktuelle Actionstates des zur Zeit ausgeweahlten States\n",
    "                    values = state.actionvalues\n",
    "                    # Erhoehung des Stepcounters\n",
    "                    steps += 1\n",
    "                    # Pruefung ob der aktuelle State das Ziel ist oder nicht (wenn nur 0, dann Ziel)\n",
    "                    if np.all(values):\n",
    "                        # Erhoehung der Col, wenn 'rechts (0)' vom Agent ausgewaehlt wurde\n",
    "                        if action == 0:\n",
    "                            ccol += 1\n",
    "                        # Erhoehung der Row, wenn 'runter (1)' vom Agent ausgewaehlt wurde\n",
    "                        elif action == 1:\n",
    "                            crow += 1\n",
    "                        # Dezimierung der Col, wenn 'links (2)' vom Agent ausgewaehlt wurde\n",
    "                        elif action == 2:\n",
    "                            ccol -= 1\n",
    "                        # Dezimierung der Row, wenn 'rauf (3)' vom Agent ausgewaehlt wurde\n",
    "                        elif action == 3:\n",
    "                            crow -= 1\n",
    "                        # Ueberschreibung des States mit den neuen Koordinaten\n",
    "                        state = self.agent.stateinfos.get(self.environent.coord2state(crow, ccol))                        \n",
    "                    else:\n",
    "                        goal = True\n",
    "                    # Wenn die Steps zu groß werden, befindet sich der Agent in einem Deadlock und muss abbrechen\n",
    "                    if steps > 21:\n",
    "                        dead = True                        \n",
    "                else:\n",
    "                    dead = True\n",
    "            # Wenn dead, dann Index Value = -1\n",
    "            if dead:\n",
    "                result[row][col] = -1\n",
    "            else:\n",
    "                result[row][col] = steps\n",
    "            print(\"row: {} | col: {} | needs {} steps\".format(row, col, steps))\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e728d8e-a57a-4dcf-bd22-df02ed6e427c",
   "metadata": {},
   "source": [
    "Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17367f71-8563-4540-8f92-47058510d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepcounts = [ \n",
    " [ 5,  4,  3,  2,  1, -2,  1,  2,  3,  4,  5,  6,  7,  8, -1, -1, -1],\n",
    " [ 6,  5,  4,  3,  2,  1,  2,  3,  4,  5,  6,  7, 10, -1, -1, -1, -1],\n",
    " [ 7,  6, -1,  4,  3,  2,  3,  4,  5,  6,  7,  8,  9, 10, -1, -1, -1],\n",
    " [ 8,  7, -1,  5, -1, -1, -1, -1, -1,  7, -1, -1, -1, -1, -1, -1, -1],\n",
    " [ 9,  8, -1,  6,  7,  8,  9, 10,  9,  8,  9, 10, -1, -1, -1, -1, -1],\n",
    " [10,  9, -1,  7,  8,  9, 10, 11, 10,  9, 10, 11, -1, -1, -1, -1, -1],\n",
    " [11, 10, -1,  8,  9, 10, 11, 12, 11, 10, 11, -1, -1, -1, -1, -1, -1],\n",
    " [12, 11, -1,  9, 10, 11, 12, 13, 12, 11, -1, -1, -1, -1, -1, -1, -1]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb37959-e0a6-4b50-a9e1-daffeb55046d",
   "metadata": {},
   "source": [
    "Viele der Schritte sind schon sehr nah am optimalen Wert dran. Allerdings gibt es gerade im unteren rechten Bereich noch sehr viele **-1** Werte. Das liegt daran, dass aufgrund der 'Greedy-Q-Policy' nicht jedes Gebiet ausgiebig erforscht wird. Würde man den Agenten weitere Episoden durchlaufen lassen, würden auch hier akurate Schrittzahlen erreicht werden können.\n",
    "Aus diesem Grund entstehen hier einige Deadlocks, von denen aus der Agent nicht zu seinem Ziel finden kann."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}